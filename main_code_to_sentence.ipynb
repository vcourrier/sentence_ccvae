{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "#import math\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "#from torchvision.utils import make_grid, save_image\n",
    "import torch.distributions as dist\n",
    "import os\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review sentiment\n",
      "0  One of the other reviewers has mentioned that ...  positive\n",
      "1  A wonderful little production. <br /><br />The...  positive\n",
      "2  I thought this was a wonderful way to spend ti...  positive\n",
      "3  Basically there's a family where a little boy ...  negative\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...  positive \n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"/home/vcourrier/these_code/mva_project/data/IMDB Dataset.csv\")\n",
    "print(df.head(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/vcourrier/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/vcourrier/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>Fun, entertaining movie about WWII German spy ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>Give me a break. How can anyone say that this ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>This movie is a bad movie. But after watching ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>This is a movie that was probably made to ente...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>Smashing film about film-making. Shows the int...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 review sentiment\n",
       "0     One of the other reviewers has mentioned that ...  positive\n",
       "1     A wonderful little production. <br /><br />The...  positive\n",
       "2     I thought this was a wonderful way to spend ti...  positive\n",
       "3     Basically there's a family where a little boy ...  negative\n",
       "4     Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "...                                                 ...       ...\n",
       "9995  Fun, entertaining movie about WWII German spy ...  positive\n",
       "9996  Give me a break. How can anyone say that this ...  negative\n",
       "9997  This movie is a bad movie. But after watching ...  negative\n",
       "9998  This is a movie that was probably made to ente...  negative\n",
       "9999  Smashing film about film-making. Shows the int...  positive\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "#stop_words = set(stopwords.words('english'))\n",
    "#stemmer = PorterStemmer()\n",
    "\n",
    "df_tok = df[:10000].copy() \n",
    "\n",
    "df_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized\n"
     ]
    }
   ],
   "source": [
    "# Tokenize, remove stopwords, and stem\n",
    "df_tok['review'] = df_tok['review'].apply(lambda x: word_tokenize(x.lower()))\n",
    "print(\"Tokenized\")\n",
    "#df_tok['review'] = df_tok['review'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "#print(\"Removed stopwords\")\n",
    "#df_tok['review'] = df_tok['review'].apply(lambda x: [stemmer.stem(word) for word in x])\n",
    "#print(\"Stemmed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit vocabulary size\n",
    "vocab_size = 10000\n",
    "all_words = [word for review in df_tok['review'] for word in review]\n",
    "word_counts = Counter(all_words)\n",
    "most_common_words = set([word for word, count in word_counts.most_common(vocab_size)])\n",
    "\n",
    "df_tok['review'] = df_tok['review'].apply(lambda x: [word if word in most_common_words else '<UNK>' for word in x])\n",
    "\n",
    "# Encode reviews\n",
    "word2value = {word: idx for idx, word in enumerate(most_common_words, start=1)}\n",
    "word2value['<UNK>'] = 0\n",
    "\n",
    "df_enc = df_tok.copy()\n",
    "df_enc['review'] = df_enc['review'].apply(lambda x: [word2value[word] for word in x])\n",
    "\n",
    "# Convert to tensors and pad\n",
    "review_tensors = [torch.tensor(encoded_review) for encoded_review in df_enc['review']]\n",
    "padded = pad_sequence(review_tensors, batch_first=True, padding_value=0).narrow(1, 0, 379)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 review  sentiment\n",
      "0     [1312, 2766, 1260, 7220, 6471, 9947, 8812, 282...          1\n",
      "1     [4990, 4865, 6912, 9453, 1353, 5642, 8547, 447...          1\n",
      "2     [8017, 9985, 9101, 2792, 4990, 4865, 1166, 450...          1\n",
      "3     [9033, 2471, 5095, 4990, 6055, 7056, 4990, 691...          0\n",
      "4     [0, 2433, 5095, 1139, 9470, 5914, 1260, 1526, ...          1\n",
      "...                                                 ...        ...\n",
      "9995  [7682, 6903, 7091, 2171, 3993, 9762, 4202, 425...          1\n",
      "9996  [2023, 5609, 4990, 1708, 1353, 1421, 1667, 214...          0\n",
      "9997  [9101, 2171, 1164, 4990, 5268, 2171, 1353, 576...          0\n",
      "9998  [9101, 1164, 4990, 2171, 2829, 2792, 2236, 740...          0\n",
      "9999  [6285, 1777, 3993, 3408, 1353, 6021, 1260, 676...          1\n",
      "\n",
      "[10000 rows x 2 columns]\n",
      "Training set shape: (8500, 2)\n",
      "Testing set shape: (1500, 2)\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists to store values for the DataFrame\n",
    "reviews = []\n",
    "sentiment_values = []\n",
    "\n",
    "# Iterate over each row of the tensor\n",
    "for i in range(padded.size(0)):\n",
    "    # Extract the row from the tensor\n",
    "    row = padded[i]\n",
    "    \n",
    "    # Convert the tensor row to a list and append it to the 'reviews' list\n",
    "    reviews.append(row.tolist())\n",
    "    # Extract the corresponding value of 'sentiment' column from the other DataFrame\n",
    "    sentiment_value = df_tok.iloc[i]['sentiment']\n",
    "    # Append the value to the 'sentiment_values' list\n",
    "    if sentiment_value == \"positive\" :\n",
    "        sentiment_values.append(1)\n",
    "    else :\n",
    "        sentiment_values.append(0)\n",
    "\n",
    "# Final dataframe\n",
    "final_df = pd.DataFrame({'review': reviews, 'sentiment': sentiment_values})\n",
    "\n",
    "print(final_df)\n",
    "\n",
    "\n",
    "####### Train-test split ########\n",
    "\n",
    "# Split the data into training and testing sets (80% training, 20% testing)\n",
    "train_df, test_df = train_test_split(final_df, test_size=0.15, random_state=42)\n",
    "\n",
    "print(\"Training set shape:\", train_df.shape)\n",
    "print(\"Testing set shape:\", test_df.shape)\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Extract features and labels for a single row\n",
    "        features = torch.tensor(self.dataframe.iloc[idx]['review'], dtype=torch.float32) \n",
    "        label = torch.tensor(self.dataframe.iloc[idx]['sentiment'], dtype=torch.long)  \n",
    "        return features, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batch size and number of epochs\n",
    "batch_size = 200\n",
    "\n",
    "# Create custom datasets\n",
    "train_dataset = CustomDataset(train_df)\n",
    "test_dataset = CustomDataset(test_df)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "data_train = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42.5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)/batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blocs du modèle\n",
    "\n",
    "class View(nn.Module):\n",
    "    def __init__(self, size):\n",
    "        super(View, self).__init__()\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, tensor):\n",
    "        return tensor.view(self.size)\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, z_dim, hidden_dim, bidirectional=True):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.hidden_factor = 2 if bidirectional else 1\n",
    "\n",
    "        self.encoder = nn.LSTM(input_size, hidden_dim, bidirectional=True, batch_first=True)\n",
    "\n",
    "        self.locs = nn.Linear(hidden_dim*self.hidden_factor, z_dim)\n",
    "        self.scales = nn.Linear(hidden_dim*self.hidden_factor, z_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        # x should be of shape [batch_size, seq_len, input_size]\n",
    "        if x.dim() == 1:\n",
    "            x = x.unsqueeze(0).unsqueeze(0)  # shape [1, 1, input_size]\n",
    "        elif x.dim() == 2:\n",
    "            x = x.unsqueeze(1)  # shape [batch_size, 1, input_size]\n",
    "        \n",
    "        output, (hidden, c_n) = self.encoder(x)\n",
    "        #print(\"pre hidden\", h_n.shape)\n",
    "        #hidden = h_n[-1]\n",
    "        hidden = hidden.view(batch_size, self.hidden_dim*self.hidden_factor)\n",
    "\n",
    "        locs = self.locs(hidden)\n",
    "        scales = torch.clamp(F.softplus(self.scales(hidden)), min=1e-3)\n",
    "        return locs, scales\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, z_dim, hidden_dim, vocab_size, bidirectional=True):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.input_size = input_size\n",
    "        self.bidirectional = bidirectional\n",
    "        self.hidden_factor = 2 if bidirectional else 1\n",
    "\n",
    "        # Linear layer to transform z_dim to hidden_dim\n",
    "        self.linear = nn.Linear(z_dim, hidden_dim * self.hidden_factor)\n",
    "\n",
    "        # LSTM to generate sequences\n",
    "        self.lstm = nn.LSTM(input_size, hidden_dim, bidirectional=bidirectional, batch_first=True)\n",
    "        self.output = nn.Linear(hidden_dim * self.hidden_factor, vocab_size)\n",
    "\n",
    "    def forward(self, z, xs):\n",
    "        batch_size = z.size(0)\n",
    "        seq_len = xs.size(1)\n",
    "\n",
    "        # Transform latent vector to initial hidden state\n",
    "        hidden = self.linear(z).view(self.hidden_factor, batch_size, self.hidden_dim)\n",
    "\n",
    "        if xs.is_cuda:\n",
    "            hidden = hidden.cuda()\n",
    "\n",
    "        c_0 = torch.zeros(self.hidden_factor, batch_size, self.hidden_dim)\n",
    "        if xs.is_cuda:\n",
    "            c_0 = c_0.cuda()\n",
    "\n",
    "        decoder_hidden = (hidden, c_0)\n",
    "\n",
    "        # Forward pass through LSTM\n",
    "        outputs, _ = self.lstm(xs, decoder_hidden)\n",
    "        print('not crashed yet')\n",
    "\n",
    "         # Print the shape of outputs before the linear layer\n",
    "        print(f\"Shape of LSTM outputs: {outputs.shape}\")\n",
    "\n",
    "        # Ensure the shape matches the expected input for the linear layer\n",
    "        expected_shape = (batch_size, seq_len, self.hidden_dim * self.hidden_factor)\n",
    "        if outputs.shape != expected_shape:\n",
    "            print(f\"Unexpected shape: {outputs.shape}, expected: {expected_shape}\")\n",
    "            return None  # or handle appropriately\n",
    "        # Project each time step's output to the vocabulary size\n",
    "        outputs = self.output(outputs)\n",
    "        print('crashed?')\n",
    "        # Apply log softmax to get log probabilities over the vocabulary\n",
    "        logp = nn.functional.log_softmax(outputs, dim=-1)\n",
    "        return logp\n",
    "\n",
    "class Diagonal(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(Diagonal, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.weight = nn.Parameter(torch.ones(self.dim))\n",
    "        self.bias = nn.Parameter(torch.zeros(self.dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.weight + self.bias\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.diag = Diagonal(self.dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.diag(x)\n",
    "\n",
    "class CondPrior(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(CondPrior, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.diag_loc_true = nn.Parameter(torch.zeros(self.dim))\n",
    "        self.diag_loc_false = nn.Parameter(torch.zeros(self.dim))\n",
    "        self.diag_scale_true = nn.Parameter(torch.ones(self.dim))\n",
    "        self.diag_scale_false = nn.Parameter(torch.ones(self.dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1) \n",
    "        loc = x * self.diag_loc_true + (1 - x) * self.diag_loc_false\n",
    "        scale = x * self.diag_scale_true + (1 - x) * self.diag_scale_false\n",
    "        return loc, torch.clamp(F.softplus(scale), min=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CCVAE model\n",
    "\n",
    "def compute_kl(locs_q, scale_q, locs_p=None, scale_p=None):\n",
    "    \"\"\"\n",
    "    Computes the KL(q||p)\n",
    "    \"\"\"\n",
    "    if locs_p is None:\n",
    "        locs_p = torch.zeros_like(locs_q)\n",
    "    if scale_p is None:\n",
    "        scale_p = torch.ones_like(scale_q)\n",
    "\n",
    "    dist_q = dist.Normal(locs_q, scale_q)\n",
    "    dist_p = dist.Normal(locs_p, scale_p)\n",
    "    return dist.kl.kl_divergence(dist_q, dist_p).sum(dim=-1)\n",
    "\n",
    "def img_log_likelihood(recon, xs):\n",
    "        if xs.dim() == 1:\n",
    "            xs = xs.unsqueeze(0).unsqueeze(0)  # shape [1, 1, input_size]\n",
    "            recon = recon.unsqueeze(0).unsqueeze(0)\n",
    "        elif xs.dim() == 2:\n",
    "            xs = xs.unsqueeze(1)  # shape [batch_size, 1, input_size]\n",
    "            recon = recon.unsqueeze(1)\n",
    "        laplace_dist = dist.Laplace(recon, torch.ones_like(recon))\n",
    "        log_prob = laplace_dist.log_prob(xs)\n",
    "        return log_prob.sum(dim=(0, 1, 2))\n",
    "\n",
    "def sentence_log_likelihood(recon, xs):\n",
    "    if xs.dim() == 2:\n",
    "        xs = xs.unsqueeze(1)  \n",
    "        recon = recon.unsqueeze(1)\n",
    "\n",
    "    normal_dist = dist.Normal(recon, torch.ones_like(recon))  \n",
    "    log_prob = normal_dist.log_prob(xs)\n",
    "    return log_prob.sum(dim=(1, 2))\n",
    "\n",
    "class CCVAE(nn.Module):\n",
    "    \"\"\"\n",
    "    CCVAE\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_dim, vocab_size, z_dim, num_classes, use_cuda, prior_fn):\n",
    "        super(CCVAE, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.input_size = input_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.z_classify = num_classes\n",
    "        self.z_style = z_dim - num_classes\n",
    "        self.use_cuda = use_cuda\n",
    "        self.num_classes = num_classes\n",
    "        self.ones = torch.ones(1, self.z_style)\n",
    "        self.zeros = torch.zeros(1, self.z_style)\n",
    "        self.y_prior_params = prior_fn\n",
    "\n",
    "        self.classifier = Classifier(self.num_classes)\n",
    "\n",
    "        self.encoder = Encoder(self.input_size, self.z_dim, self.hidden_dim)\n",
    "        self.decoder = Decoder(self.input_size, self.z_dim, self.hidden_dim, self.vocab_size)\n",
    "\n",
    "        self.cond_prior = CondPrior(self.num_classes)\n",
    "\n",
    "        if self.use_cuda:\n",
    "            self.ones = self.ones.cuda()\n",
    "            self.zeros = self.zeros.cuda()\n",
    "            self.y_prior_params = self.y_prior_params.cuda()\n",
    "            self.cuda()\n",
    "\n",
    "    def sup(self, x, y):\n",
    "        y = y.float()\n",
    "        bs = x.shape[0]\n",
    "        #inference\n",
    "        post_params = self.encoder(x)\n",
    "        \n",
    "        z = dist.Normal(*post_params).rsample()\n",
    "        zc, zs = z.split([self.z_classify, self.z_style], 1)\n",
    "        qyzc = dist.Bernoulli(logits=self.classifier(zc))\n",
    "        log_qyzc = qyzc.log_prob(y).sum(dim=-1)\n",
    "\n",
    "        # compute kl\n",
    "        locs_p_zc, scales_p_zc = self.cond_prior(y)\n",
    "        prior_params = (torch.cat([locs_p_zc, self.zeros.expand(bs, -1)], dim=1), \n",
    "                        torch.cat([scales_p_zc, self.ones.expand(bs, -1)], dim=1))\n",
    "        kl = compute_kl(*post_params, *prior_params)\n",
    "\n",
    "        #compute log probs for x and y\n",
    "\n",
    "        log_py = dist.Bernoulli(self.y_prior_params.expand(bs, -1)).log_prob(y).sum(dim=-1)\n",
    "        print('not crashed')\n",
    "        recon = self.decoder(z, x)\n",
    "        print('crashed?')\n",
    "        \n",
    "\n",
    "        print('here?')\n",
    "        log_qyx = self.classifier_loss(x, y)\n",
    "        #recon = recon.transpose(0, 1)\n",
    "        #recon = recon.reshape(bs, -1, 1)  # Transpose back to [batch_size, seq_len, hidden_dim]\n",
    "        #log_pxz = sentence_log_likelihood(recon, x)\n",
    "        \n",
    "        NLL = nn.NLLLoss(ignore_index=0, reduction='sum')  # Assuming 0 is the padding index\n",
    "        log_pxz = NLL(recon.view(-1, self.vocab_size), y.view(-1))\n",
    "        print('crashed')\n",
    "\n",
    "        # we only want gradients wrt to params of qyz, so stop them propogating to qzx\n",
    "        log_qyzc_ = dist.Bernoulli(logits=self.classifier(zc.detach())).log_prob(y).sum(dim=-1)\n",
    "        w = torch.exp(log_qyzc_ - log_qyx)+ 1e-8\n",
    "        elbo = (w * (log_pxz - kl - log_qyzc) + log_py + log_qyx).mean()\n",
    "        return -elbo\n",
    "\n",
    "    def classifier_loss(self, x, y, k=100):\n",
    "        \"\"\"\n",
    "        Computes the classifier loss.\n",
    "        \"\"\"\n",
    "        zc, _ = dist.Normal(*self.encoder(x)).rsample(torch.tensor([k])).split([self.z_classify, self.z_style], -1)\n",
    "        logits = self.classifier(zc.view(-1, self.z_classify))\n",
    "        d = dist.Bernoulli(logits=logits)\n",
    "        y = y.unsqueeze(0).unsqueeze(-1)\n",
    "        y = y.expand(k, -1, -1).contiguous().view(-1, self.num_classes)\n",
    "        lqy_z = d.log_prob(y).view(k, x.shape[0], self.num_classes).sum(dim=-1)\n",
    "        lqy_x = torch.logsumexp(lqy_z, dim=0) - np.log(k)\n",
    "        return lqy_x\n",
    "\n",
    "    #def reconstruct_img(self, x):\n",
    "    #    return self.decoder(dist.Normal(*self.encoder(x)).rsample())\n",
    "\n",
    "    def classifier_acc(self, x, y=None, k=1):\n",
    "        zc, _ = dist.Normal(*self.encoder(x)).rsample(torch.tensor([k])).split([self.z_classify, self.z_style], -1)\n",
    "        logits = self.classifier(zc.view(-1, self.z_classify)).view(-1, self.num_classes)\n",
    "        y = y.unsqueeze(0).unsqueeze(-1)\n",
    "        y = y.expand(k, -1, -1).contiguous().view(-1, self.num_classes)\n",
    "        preds = torch.round(torch.sigmoid(logits))\n",
    "        acc = (preds.eq(y)).float().mean()\n",
    "        return acc\n",
    "    \n",
    "    def accuracy(self, data_loader, *args, **kwargs):\n",
    "        acc = 0.0\n",
    "        for (x, y) in data_loader:\n",
    "            if self.use_cuda:\n",
    "                x, y = x.cuda(), y.cuda()\n",
    "            batch_acc = self.classifier_acc(x, y)\n",
    "            acc += batch_acc\n",
    "        return acc / len(data_loader)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 1 \n",
    "seq_len = 379\n",
    "hidden_dim = 256\n",
    "z_dim = 28\n",
    "num_classes = 1\n",
    "use_cuda = False\n",
    "prior_fn = torch.tensor([[0.5]])\n",
    "n_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_batch = int(len(train_dataset)/batch_size)\n",
    "num_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.LSTM):\n",
    "        for name, param in m.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.zeros_(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "0\n",
      "not crashed\n",
      "not crashed yet\n",
      "Shape of LSTM outputs: torch.Size([200, 379, 512])\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "# Training\n",
    "cc_vae = CCVAE(input_size, hidden_dim, vocab_size, z_dim, num_classes, use_cuda, prior_fn)\n",
    "\n",
    "cc_vae.apply(init_weights)\n",
    "\n",
    "optim = torch.optim.Adam(params=cc_vae.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(\"Epoch:\", epoch)\n",
    "    cc_vae.train()\n",
    "    \n",
    "    epoch_losses_sup = 0.0\n",
    "    #for i in tqdm(range(num_batch)):\n",
    "    for batch_idx, (xs, ys) in enumerate(train_loader):\n",
    "        print(batch_idx)\n",
    "        xs = xs.view(xs.shape[0], seq_len, input_size)\n",
    "        loss = cc_vae.sup(xs, ys)\n",
    "        \n",
    "        optim.zero_grad() \n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        epoch_losses_sup += loss.detach().item()\n",
    "        print(f\"Batch {batch_idx} Loss: {loss.detach().item():.3f}\")\n",
    "        gc.collect()\n",
    "    \n",
    "    avg_loss = epoch_losses_sup / len(train_loader)\n",
    "    print(f\"[Epoch {epoch+1:03d}] Sup Loss: {avg_loss:.3f}\")\n",
    "\n",
    "cc_vae.eval() \n",
    "test_acc = cc_vae.accuracy(test_loader)\n",
    "\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mva_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
