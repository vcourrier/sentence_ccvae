{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/vcourrier/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/vcourrier/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/vcourrier/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "#import math\n",
    "import matplotlib.pyplot as plt\n",
    "from ccvae_model import *\n",
    "from process_data import *\n",
    "from m2_model import *\n",
    "\n",
    "import numpy as np\n",
    "#from torchvision.utils import make_grid, save_image\n",
    "import torch.distributions as dist\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim as optim\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review sentiment\n",
      "0  One of the other reviewers has mentioned that ...  positive\n",
      "1  A wonderful little production. <br /><br />The...  positive\n",
      "2  I thought this was a wonderful way to spend ti...  positive\n",
      "3  Basically there's a family where a little boy ...  negative\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...  positive \n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/IMDB_Dataset.csv\")\n",
    "print(df.head(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (42500, 2)\n",
      "Testing set shape: (7500, 2)\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10000\n",
    "seq_len = 100\n",
    "\n",
    "final_df, word2value = process_IMDB(\"data/IMDB_Dataset.csv\", vocab_size = vocab_size, max_pad=seq_len)\n",
    "\n",
    "####### Train-test split ########\n",
    "\n",
    "# Split the data into training and testing sets (80% training, 20% testing)\n",
    "train_df, test_df = train_test_split(final_df, test_size=0.15, random_state=42)\n",
    "\n",
    "print(\"Training set shape:\", train_df.shape)\n",
    "print(\"Testing set shape:\", test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batch size and number of epochs\n",
    "batch_size = 200\n",
    "\n",
    "# Create custom datasets\n",
    "train_dataset = CustomDataset(train_df)\n",
    "test_dataset = CustomDataset(test_df)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "data_train = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 1 \n",
    "hidden_dim = 256\n",
    "z_dim = 28\n",
    "num_classes = 1\n",
    "prior_fn = torch.tensor([[0.5]])\n",
    "n_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "212"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_batch = int(len(train_dataset)/batch_size)\n",
    "num_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.LSTM):\n",
    "        for name, param in m.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.zeros_(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "  Batch: 0\n",
      "Batch 0 Loss: 20.755\n",
      "  Batch: 1\n",
      "Batch 1 Loss: 19.612\n",
      "  Batch: 2\n",
      "Batch 2 Loss: 19.539\n",
      "  Batch: 3\n",
      "Batch 3 Loss: 19.414\n",
      "  Batch: 4\n",
      "Batch 4 Loss: 19.211\n",
      "  Batch: 5\n",
      "Batch 5 Loss: 19.195\n",
      "  Batch: 6\n",
      "Batch 6 Loss: 18.643\n",
      "  Batch: 7\n",
      "Batch 7 Loss: 19.083\n",
      "  Batch: 8\n",
      "Batch 8 Loss: 19.013\n",
      "  Batch: 9\n",
      "Batch 9 Loss: 17.426\n",
      "  Batch: 10\n",
      "Batch 10 Loss: 17.308\n",
      "  Batch: 11\n",
      "Batch 11 Loss: 16.716\n",
      "  Batch: 12\n",
      "Batch 12 Loss: 17.119\n",
      "  Batch: 13\n",
      "Batch 13 Loss: 18.053\n",
      "  Batch: 14\n",
      "Batch 14 Loss: 16.453\n",
      "  Batch: 15\n",
      "Batch 15 Loss: 15.370\n",
      "  Batch: 16\n",
      "Batch 16 Loss: 15.892\n",
      "  Batch: 17\n",
      "Batch 17 Loss: 14.507\n",
      "  Batch: 18\n",
      "Batch 18 Loss: 15.216\n",
      "  Batch: 19\n",
      "Batch 19 Loss: 15.513\n",
      "  Batch: 20\n",
      "Batch 20 Loss: 15.383\n",
      "  Batch: 21\n",
      "Batch 21 Loss: 15.155\n",
      "  Batch: 22\n",
      "Batch 22 Loss: 14.869\n",
      "  Batch: 23\n",
      "Batch 23 Loss: 13.746\n",
      "  Batch: 24\n",
      "Batch 24 Loss: 14.453\n",
      "  Batch: 25\n",
      "Batch 25 Loss: 14.371\n",
      "  Batch: 26\n",
      "Batch 26 Loss: 13.404\n",
      "  Batch: 27\n",
      "Batch 27 Loss: 14.067\n",
      "  Batch: 28\n",
      "Batch 28 Loss: 13.583\n",
      "  Batch: 29\n",
      "Batch 29 Loss: 13.563\n",
      "  Batch: 30\n",
      "Batch 30 Loss: 12.864\n",
      "  Batch: 31\n",
      "Batch 31 Loss: 12.934\n",
      "  Batch: 32\n",
      "Batch 32 Loss: 13.335\n",
      "  Batch: 33\n",
      "Batch 33 Loss: 13.147\n",
      "  Batch: 34\n",
      "Batch 34 Loss: 12.716\n",
      "  Batch: 35\n",
      "Batch 35 Loss: 12.936\n",
      "  Batch: 36\n",
      "Batch 36 Loss: 11.786\n",
      "  Batch: 37\n",
      "Batch 37 Loss: 11.561\n",
      "  Batch: 38\n",
      "Batch 38 Loss: 11.510\n",
      "  Batch: 39\n",
      "Batch 39 Loss: 11.890\n",
      "  Batch: 40\n",
      "Batch 40 Loss: 11.294\n",
      "  Batch: 41\n",
      "Batch 41 Loss: 11.958\n",
      "  Batch: 42\n",
      "Batch 42 Loss: 11.077\n",
      "[Epoch 001] Sup Loss: 15.247\n",
      "Epoch: 1\n",
      "  Batch: 0\n",
      "Batch 0 Loss: 11.764\n",
      "  Batch: 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m cc_vae\u001b[38;5;241m.\u001b[39msup(xs, ys)\n\u001b[1;32m     21\u001b[0m optim\u001b[38;5;241m.\u001b[39mzero_grad() \n\u001b[0;32m---> 22\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m optim\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     25\u001b[0m epoch_losses_sup \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.pyenv/versions/mva_project/lib/python3.11/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/mva_project/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/mva_project/lib/python3.11/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "# Training\n",
    "cc_vae = CCVAE(input_size, hidden_dim, vocab_size, z_dim, num_classes, use_cuda, prior_fn, device).to(device=device)\n",
    "\n",
    "cc_vae.apply(init_weights)\n",
    "\n",
    "optim = torch.optim.Adam(params=cc_vae.parameters(), lr=0.0001)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    cc_vae.train()\n",
    "    \n",
    "    epoch_losses_sup = 0.0\n",
    "    for batch_idx, (_, xs, ys) in enumerate(train_loader):\n",
    "        xs = xs.view(xs.shape[0], seq_len, input_size).to(device)\n",
    "        ys = ys.to(device)\n",
    "        loss = cc_vae.sup(xs, ys)\n",
    "        \n",
    "        optim.zero_grad() \n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        epoch_losses_sup += loss.detach().item()\n",
    "        #print(f\"Batch {batch_idx} Loss: {loss.detach().item():.3f}\")\n",
    "        gc.collect()\n",
    "    \n",
    "    avg_loss = epoch_losses_sup / len(train_loader)\n",
    "    print(f\"[Epoch {epoch+1:03d}] Sup Loss: {avg_loss:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4944, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "#Accuracy\n",
    "cc_vae.eval() \n",
    "\n",
    "test_acc = cc_vae.accuracy(test_loader)\n",
    "\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intervention\n",
    "\n",
    "def intervention(model, x, y):\n",
    "    post_params = model.encoder(x)\n",
    "    z = dist.Normal(*post_params).rsample()\n",
    "    zc, zs = z.split([model.z_classify, model.z_style], 1)\n",
    "    locs_p_zc, scales_p_zc = model.cond_prior(1 - y)\n",
    "    zc_new = dist.Normal(locs_p_zc, scales_p_zc).rsample()\n",
    "    z_new = torch.cat((zc_new,zs),dim=1)\n",
    "    recons = model.decoder(z_new,x)\n",
    "    return recons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "value2word = {v: k for k, v in word2value.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_recons(recons):\n",
    "    recons = recons.round()\n",
    "    r = torch.zeros(recons.shape[1])\n",
    "    for i in range(recons.shape[1]):\n",
    "        r[i] = torch.argmax(recons[0][i]) \n",
    "    decoded_review = [value2word.get(idx.item(), '<UNK>') for idx in r]\n",
    "    decoded_text = '.'.join(decoded_review)\n",
    "    return decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True review ['this', 'is', 'a', 'pretty', 'pointless', 'remake', '.', 'starting', 'with', 'the', 'opening', 'title', 'shots', 'of', 'the', 'original', 'was', 'a', 'real', 'mistake', 'as', 'it', 'reminds', 'the', 'viewer', 'of', 'what', 'a', 'great', 'little', 'period', 'piece', '<UNK>', 'that', 'was', '.', 'the', 'new', 'version', 'that', 'follows', 'is', 'an', 'exercise', 'in', '<UNK>', '<', 'br', '/', '>', '<', 'br', '/', '>', 'brian', '<UNK>', 'plays', 'a', '<UNK>', 'boy', \"'\", 'photographer', 'who', 'returns', 'to', 'a', '<UNK>', 'desert', 'town', 'populated', 'by', 'a', '<UNK>', 'of', '<UNK>', 'clich√©d', 'stock', 'characters', ':', 'the', '<UNK>', 'sucking', '<UNK>', '<UNK>', ',', 'the', '<UNK>', 'old', '<UNK>', '<UNK>', ',', 'the', 'crippled', 'vet', 'and', 'his', 'asian', 'wife', ',', 'etc', '...', '<', 'br', '/', '>', '<', 'br', '/', '>', '<UNK>', \"'s\", 'character', 'witnesses', 'the', 'crashing', 'of', '<UNK>', \"'\", 'into', 'a', '<UNK>', 'and', 'shortly', 'after', 'strange', 'things', 'start', 'to', 'happen', 'as', 'pieces', 'of', 'weird', 'blue', 'rock', 'are', 'scattered', 'around', '.', 'the', '<UNK>', 'starts', 'to', 'rise', ',', 'all', 'the', 'water', 'in', 'the', 'area', 'vanishes', ',', 'people', 'start', 'to', 'act', '<UNK>', ',', 'things', 'explode', '.', '<UNK>', \"'s\", 'character', 'gets', 'in', 'and', 'out', 'of', 'his', 'car', 'more', 'often', 'than', 'is', '<UNK>', 'possible', 'in', 'one', 'movie', '.', 'the', 'film', 'develops', 'no', 'sense', 'of', 'place', ',', 'no', 'character', 'development', ',', 'no', 'humour', ',', 'no', 'tension', '.', 'everything', 'that', 'made', 'the', 'jack', 'arnold', \"'s\", 'original', 'a', 'creepy', 'little', '<UNK>', 'paranoia', 'classic', 'has', 'been', 'abandoned', '.', 'it', 'just', 'runs', 'through', 'its', 'minimal', '<UNK>', 'and', 'then', 'just', '<UNK>', '<', 'br', '/', '>', '<', 'br', '/', '>', 'the', 'special', 'effects', 'are', \"n't\", 'very', 'special', '-', 'the', 'interior', 'of', 'the', 'ship', 'looks', 'like', 'bits', 'of', '<UNK>', 'film', 'wrapped', 'round', 'some', '<UNK>', 'which', 'were', 'then', '<UNK>', 'in', 'front', 'of', 'the', 'camera', 'to', 'frame', 'some', 'of', 'the', 'most', 'uninspired', 'and', 'clumsy', '<UNK>', 'ever', 'put', 'onto', 'the', 'screen', '.', 'the', 'script', 'is', 'repetitive', '-', 'everyone', 'says', 'everything', 'at', 'least', 'twice', ',', '<UNK>', 'gets', 'to', 'say', '``', 'let', \"'s\", 'get', 'out', 'of', 'here', \"''\", 'at', 'least', 'three', 'times', 'during', 'the', 'movie', ',', 'twice', 'in', 'one', 'scene', '.', 'loads', 'of', 'things', 'are', 'left', 'unexplained', 'at', 'the', 'end', '-', 'why', 'do', 'the', 'aliens', 'need', 'all', 'the', 'heat', 'and', 'water', 'for', 'example', '?', '-', 'not', 'that', 'anyone', 'watching', 'would', 'care', ';', 'if', 'the', 'film', 'makers', 'did', \"n't\", 'care', 'why', 'should', 'we', '?', '<', 'br', '/', '>', '<', 'br', '/', '>', 'the', 'acting', 'is', 'adequate', '-', 'better', 'than', 'the', 'script', ',', 'which', 'at', 'times', ',', 'has', 'an', '<UNK>', '<UNK>', 'quality', ',', 'deserves', '.', 'though', 'often', 'the', 'actors', 'look', 'like', 'they', 'just', 'want', 'to', 'get', 'the', 'thing', 'over', 'with', 'as', 'quickly', 'as', 'possible', '-', 'a', 'notable', 'example', 'of', 'this', 'is', 'when', 'elizabeth', '<UNK>', '<UNK>', 'the', '<UNK>', ',', 'token', 'moment', 'of', '``', 'frustrated', 'despair', 'hands', 'to', 'face', '<UNK>', \"''\", 'before', 'following', '<UNK>', 'son', '<UNK>', 'outside', 'to', 'watch', 'him', 'do', '``', 'angry', '<UNK>', 'teenager', 'smashing', 'something', 'off', 'a', 'table', \"''\", '<UNK>', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'continuity', 'errors', 'include', 'the', '(', '<UNK>', ')', '<UNK>', 'on', 'the', 'back', 'of', '<UNK>', \"'s\", 'jeep', 'appearing', 'and', 'disappearing', ',', 'a', 'double', 'action', 'of', 'the', 'gas', 'in', 'the', 'exploding', 'car', ',', 'a', '<UNK>', 'being', 'in', 'two', 'places', 'simultaneously', '-', 'once', 'in', 'the', 'alien', '<UNK>', \"'s\", '<UNK>', 'shot', 'then', 'immediately', 'afterwards', 'in', 'a', 'reaction', 'shot', ',', 'elizabeth', '<UNK>', 'appearing', 'to', 'shut', 'a', 'car', 'door', 'twice', '...', 'you', 'can', 'tell', 'i', 'was', '<UNK>', 'ca', \"n't\", 'you', '?', 'the', 'movie', 'commits', 'that', 'greatest', 'of', 'errors', '.', 'it', \"'s\", 'boring', '.']\n",
      "Modified review ,.and.,.,.,.......,.the.the.the.and.and.the...and.and.and.and.and.and...the...and.the.and.the.and.the...<UNK>.the.and...the.the.and.the...and.and.and...<UNK>.....and.......and...and.<UNK>.and.and.<UNK>.is...and.......and.<UNK>.and.and.the.and.and.<UNK>.and.<UNK>.and.is...and.the.<UNK>.and.<UNK>.<UNK>.is.the.<UNK>...<UNK>.<UNK>.,.the.and...,.,.well.,.,..\n",
      "True review ['i', 'agree', 'with', 'the', 'comments', 'regarding', 'the', '<UNK>', 'spin', '.', 'the', 'last', 'view', 'shows', 'have', 'been', 'a', 'little', 'better', ',', 'but', 'surely', 'the', 'writers', 'need', 'some', 'more', 'direction', '.', 'i', 'think', 'the', 'characters', 'are', 'still', 'interesting', ',', 'although', 'sometimes', 'they', 'spin', 'into', 'the', '``', 'white', 'trash', \"''\", 'things', 'a', 'little', 'too', 'much', '.', 'subtlety', 'and', '<UNK>', 'goes', 'a', 'long', 'way', 'on', 'shows', 'like', '``', 'office', \"''\", '.', 'i', 'would', 'think', 'the', 'target', 'audience', 'is', 'somewhat', 'similar', 'being', 'they', 'are', 'both', 'on', 'the', 'same', 'night', 'and', '<UNK>', '.', 'one', 'would', 'think', 'that', '<UNK>', 'and', 'the', 'whole', 'eastern', 'religion', 'thing', 'is', 'a', 'big', 'enough', 'topic', 'to', 'bring', 'some', 'different', 'and', 'interesting', 'shows', ',', 'but', 'they', 'only', 'scratch', 'the', 'surface', 'of', 'the', 'subject', '.', 'in', 'my', 'opinion', 'it', 'shows', 'the', 'contempt', 'that', 'many', 'people', 'have', 'in', 'hollywood', 'about', 'the', 'level', 'of', 'intelligence', 'of', 'the', 'masses', '.', 'we', 'can', 'handle', 'more', '<UNK>', 'content', '.', 'it', 'has', 'been', 'proved', 'before', 'in', 'many', 'other', 'shows', '.']\n",
      "Modified review ,...,.the.the.and.the.<UNK>.and...the.and.and.and.and...and.and...and.and.<UNK>.the.is.......the.....the.the...and...and.is.the.and.and.is.the.the...and.and...and.and.and...is.....and.<UNK>...and...and.and.and.............and.the.the.and...and...and.and.and.and.and.is.the.....and.<UNK>...the.and.the.the.<UNK>.,.the.,.,.,.,.,.,\n",
      "True review ['i', 'picked', 'up', 'this', 'movie', 'with', 'the', 'intention', 'of', 'getting', 'a', 'bad', 'zombie', 'movie', '.', 'but', 'i', 'had', 'no', 'idea', 'what', 'i', 'was', 'getting', 'myself', '<UNK>', '<', 'br', '/', '>', '<', 'br', '/', '>', 'i', 'started', 'the', 'movie', 'and', 'soon', 'i', 'had', 'been', 'pulled', 'into', 'a', 'world', 'of', 'pain', 'and', 'visual', '<UNK>', '<', 'br', '/', '>', '<', 'br', '/', '>', 'i', 'finally', 'know', 'what', 'hell', 'is', 'like', '.', 'it', \"'s\", 'this', 'movie', '.', 'for', 'eternity', '.', 'this', 'movie', 'has', 'no', 'value', '.', 'it', 'did', \"n't\", 'even', 'really', 'have', 'a', 'plot', '.', 'there', 'was', 'stuff', 'going', 'on', 'in', 'each', 'scene', 'but', 'no', 'overall', 'explanation', 'why', 'anything', '<UNK>', '<', 'br', '/', '>', '<', 'br', '/', '>', 'instead', 'of', 'watching', 'this', 'movie', 'i', 'suggest', 'that', 'you', 'line', 'the', 'nearest', '<UNK>', 'with', 'oil', 'and', 'try', 'and', 'stuff', 'as', 'many', 'bullets', 'in', 'it', 'as', 'you', 'can', '.', 'you', 'will', 'find', 'that', 'the', 'outcome', 'to', 'be', 'far', 'more', 'pleasant', 'than', 'this', 'movie.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'do', \"n't\", 'even', 'watch', 'it', '.', 'not', 'even', 'to', 'see', 'how', 'bad', 'it', 'is', '.', 'i', 'beg', 'you', '.', 'if', 'you', 'watch', 'it', ',', 'then', 'it', 'means', 'they', 'win', '.']\n",
      "Modified review ,.,.i.i.,.,.the...,.and.,.i...,...and.and.and...,.the.and.and.and.and.<UNK>.....and.......and...and.and.the.and.and.and.and.and.....the.and...and.and.and.and.<UNK>.....and.......and...,.the.and.the...and.....and.and.and.,...the.and...,.,.,...,...and.....,.and.,.,.....,.and.and.,.and...could.the.,\n"
     ]
    }
   ],
   "source": [
    "k = 0\n",
    "C = 3\n",
    "for (i,a ,b) in test_loader :\n",
    "    while k < C:\n",
    "        y = b[k].view(1).to(device)\n",
    "        x = a[k].view(1,seq_len,input_size).to(device)\n",
    "        recons = intervention(cc_vae, x, y)\n",
    "        dec = decode_recons(recons)\n",
    "        print(\"True review\", final_df.iloc[i[k].item()]['review'])\n",
    "        print(\"Modified review\", dec)\n",
    "        k += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "M2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "  Batch: 0\n",
      "Batch 0 Loss: 14.608\n",
      "  Batch: 1\n",
      "Batch 1 Loss: 14.405\n",
      "  Batch: 2\n",
      "Batch 2 Loss: 14.172\n",
      "  Batch: 3\n",
      "Batch 3 Loss: 13.987\n",
      "  Batch: 4\n",
      "Batch 4 Loss: 13.768\n",
      "  Batch: 5\n",
      "Batch 5 Loss: 13.582\n",
      "  Batch: 6\n",
      "Batch 6 Loss: 13.422\n",
      "  Batch: 7\n",
      "Batch 7 Loss: 13.232\n",
      "  Batch: 8\n",
      "Batch 8 Loss: 13.048\n",
      "  Batch: 9\n",
      "Batch 9 Loss: 12.870\n",
      "  Batch: 10\n",
      "Batch 10 Loss: 12.709\n",
      "  Batch: 11\n",
      "Batch 11 Loss: 12.556\n",
      "  Batch: 12\n",
      "Batch 12 Loss: 12.429\n",
      "  Batch: 13\n",
      "Batch 13 Loss: 12.298\n",
      "  Batch: 14\n",
      "Batch 14 Loss: 12.142\n",
      "  Batch: 15\n",
      "Batch 15 Loss: 12.011\n",
      "  Batch: 16\n",
      "Batch 16 Loss: 11.909\n",
      "  Batch: 17\n",
      "Batch 17 Loss: 11.807\n",
      "  Batch: 18\n",
      "Batch 18 Loss: 11.687\n",
      "  Batch: 19\n",
      "Batch 19 Loss: 11.545\n",
      "  Batch: 20\n",
      "Batch 20 Loss: 11.454\n",
      "  Batch: 21\n",
      "Batch 21 Loss: 11.354\n",
      "  Batch: 22\n",
      "Batch 22 Loss: 11.277\n",
      "  Batch: 23\n",
      "Batch 23 Loss: 11.133\n",
      "  Batch: 24\n",
      "Batch 24 Loss: 11.033\n",
      "  Batch: 25\n",
      "Batch 25 Loss: 10.951\n",
      "  Batch: 26\n",
      "Batch 26 Loss: 10.840\n",
      "  Batch: 27\n",
      "Batch 27 Loss: 10.782\n",
      "  Batch: 28\n",
      "Batch 28 Loss: 10.692\n",
      "  Batch: 29\n",
      "Batch 29 Loss: 10.573\n",
      "  Batch: 30\n",
      "Batch 30 Loss: 10.486\n",
      "  Batch: 31\n",
      "Batch 31 Loss: 10.390\n",
      "  Batch: 32\n",
      "Batch 32 Loss: 10.309\n",
      "  Batch: 33\n",
      "Batch 33 Loss: 10.218\n",
      "  Batch: 34\n",
      "Batch 34 Loss: 10.155\n",
      "  Batch: 35\n",
      "Batch 35 Loss: 10.088\n",
      "  Batch: 36\n",
      "Batch 36 Loss: 10.007\n",
      "  Batch: 37\n",
      "Batch 37 Loss: 9.946\n",
      "  Batch: 38\n",
      "Batch 38 Loss: 9.902\n",
      "  Batch: 39\n",
      "Batch 39 Loss: 9.847\n",
      "  Batch: 40\n",
      "Batch 40 Loss: 9.778\n",
      "  Batch: 41\n",
      "Batch 41 Loss: 9.730\n",
      "  Batch: 42\n",
      "Batch 42 Loss: 9.616\n",
      "[Epoch 001] Sup Loss: 11.599\n",
      "Epoch: 1\n",
      "  Batch: 0\n",
      "Batch 0 Loss: 9.618\n",
      "  Batch: 1\n",
      "Batch 1 Loss: 9.584\n",
      "  Batch: 2\n",
      "Batch 2 Loss: 9.526\n",
      "  Batch: 3\n",
      "Batch 3 Loss: 9.462\n",
      "  Batch: 4\n",
      "Batch 4 Loss: 9.409\n",
      "  Batch: 5\n",
      "Batch 5 Loss: 9.404\n",
      "  Batch: 6\n",
      "Batch 6 Loss: 9.344\n",
      "  Batch: 7\n",
      "Batch 7 Loss: 9.300\n",
      "  Batch: 8\n",
      "Batch 8 Loss: 9.242\n",
      "  Batch: 9\n",
      "Batch 9 Loss: 9.197\n",
      "  Batch: 10\n",
      "Batch 10 Loss: 9.165\n",
      "  Batch: 11\n",
      "Batch 11 Loss: 9.122\n",
      "  Batch: 12\n",
      "Batch 12 Loss: 9.106\n",
      "  Batch: 13\n",
      "Batch 13 Loss: 9.056\n",
      "  Batch: 14\n",
      "Batch 14 Loss: 8.997\n",
      "  Batch: 15\n",
      "Batch 15 Loss: 9.179\n",
      "  Batch: 16\n",
      "Batch 16 Loss: 8.906\n",
      "  Batch: 17\n",
      "Batch 17 Loss: 8.889\n",
      "  Batch: 18\n",
      "Batch 18 Loss: 8.823\n",
      "  Batch: 19\n",
      "Batch 19 Loss: 8.790\n",
      "  Batch: 20\n",
      "Batch 20 Loss: 8.756\n",
      "  Batch: 21\n",
      "Batch 21 Loss: 8.713\n",
      "  Batch: 22\n",
      "Batch 22 Loss: 8.665\n",
      "  Batch: 23\n",
      "Batch 23 Loss: 8.617\n",
      "  Batch: 24\n",
      "Batch 24 Loss: 8.586\n",
      "  Batch: 25\n",
      "Batch 25 Loss: 8.551\n",
      "  Batch: 26\n",
      "Batch 26 Loss: 8.528\n",
      "  Batch: 27\n",
      "Batch 27 Loss: 8.464\n",
      "  Batch: 28\n",
      "Batch 28 Loss: 8.439\n",
      "  Batch: 29\n",
      "Batch 29 Loss: 8.414\n",
      "  Batch: 30\n",
      "Batch 30 Loss: 8.357\n",
      "  Batch: 31\n",
      "Batch 31 Loss: 8.327\n",
      "  Batch: 32\n",
      "Batch 32 Loss: 8.299\n",
      "  Batch: 33\n",
      "Batch 33 Loss: 8.247\n",
      "  Batch: 34\n",
      "Batch 34 Loss: 8.218\n",
      "  Batch: 35\n",
      "Batch 35 Loss: 8.158\n",
      "  Batch: 36\n",
      "Batch 36 Loss: 8.086\n",
      "  Batch: 37\n",
      "Batch 37 Loss: 8.093\n",
      "  Batch: 38\n",
      "Batch 38 Loss: 8.021\n",
      "  Batch: 39\n",
      "Batch 39 Loss: 7.990\n",
      "  Batch: 40\n",
      "Batch 40 Loss: 8.027\n",
      "  Batch: 41\n",
      "Batch 41 Loss: 7.914\n",
      "  Batch: 42\n",
      "Batch 42 Loss: 7.954\n",
      "[Epoch 002] Sup Loss: 8.734\n",
      "Epoch: 2\n",
      "  Batch: 0\n",
      "Batch 0 Loss: 7.880\n",
      "  Batch: 1\n",
      "Batch 1 Loss: 7.848\n",
      "  Batch: 2\n",
      "Batch 2 Loss: 7.807\n",
      "  Batch: 3\n",
      "Batch 3 Loss: 7.760\n",
      "  Batch: 4\n",
      "Batch 4 Loss: 7.747\n",
      "  Batch: 5\n",
      "Batch 5 Loss: 7.716\n",
      "  Batch: 6\n",
      "Batch 6 Loss: 7.625\n",
      "  Batch: 7\n",
      "Batch 7 Loss: 7.630\n",
      "  Batch: 8\n",
      "Batch 8 Loss: 7.644\n",
      "  Batch: 9\n",
      "Batch 9 Loss: 7.593\n",
      "  Batch: 10\n",
      "Batch 10 Loss: 7.569\n",
      "  Batch: 11\n",
      "Batch 11 Loss: 7.524\n",
      "  Batch: 12\n",
      "Batch 12 Loss: 7.526\n",
      "  Batch: 13\n",
      "Batch 13 Loss: 7.443\n",
      "  Batch: 14\n",
      "Batch 14 Loss: 7.489\n",
      "  Batch: 15\n",
      "Batch 15 Loss: 7.412\n",
      "  Batch: 16\n",
      "Batch 16 Loss: 7.457\n",
      "  Batch: 17\n",
      "Batch 17 Loss: 7.373\n",
      "  Batch: 18\n",
      "Batch 18 Loss: 7.412\n",
      "  Batch: 19\n",
      "Batch 19 Loss: 7.359\n",
      "  Batch: 20\n",
      "Batch 20 Loss: 7.399\n",
      "  Batch: 21\n",
      "Batch 21 Loss: 7.341\n",
      "  Batch: 22\n",
      "Batch 22 Loss: 7.277\n",
      "  Batch: 23\n",
      "Batch 23 Loss: 7.300\n",
      "  Batch: 24\n",
      "Batch 24 Loss: 7.224\n",
      "  Batch: 25\n",
      "Batch 25 Loss: 7.205\n",
      "  Batch: 26\n",
      "Batch 26 Loss: 7.231\n",
      "  Batch: 27\n",
      "Batch 27 Loss: 7.227\n",
      "  Batch: 28\n",
      "Batch 28 Loss: 7.117\n",
      "  Batch: 29\n",
      "Batch 29 Loss: 7.203\n",
      "  Batch: 30\n",
      "Batch 30 Loss: 7.031\n",
      "  Batch: 31\n",
      "Batch 31 Loss: 7.077\n",
      "  Batch: 32\n",
      "Batch 32 Loss: 7.094\n",
      "  Batch: 33\n",
      "Batch 33 Loss: 7.068\n",
      "  Batch: 34\n",
      "Batch 34 Loss: 7.041\n",
      "  Batch: 35\n",
      "Batch 35 Loss: 7.150\n",
      "  Batch: 36\n",
      "Batch 36 Loss: 6.978\n",
      "  Batch: 37\n",
      "Batch 37 Loss: 7.019\n",
      "  Batch: 38\n",
      "Batch 38 Loss: 6.979\n",
      "  Batch: 39\n",
      "Batch 39 Loss: 6.943\n",
      "  Batch: 40\n",
      "Batch 40 Loss: 6.963\n",
      "  Batch: 41\n",
      "Batch 41 Loss: 6.976\n",
      "  Batch: 42\n",
      "Batch 42 Loss: 6.928\n",
      "[Epoch 003] Sup Loss: 7.339\n",
      "Epoch: 3\n",
      "  Batch: 0\n",
      "Batch 0 Loss: 6.944\n",
      "  Batch: 1\n",
      "Batch 1 Loss: 6.921\n",
      "  Batch: 2\n",
      "Batch 2 Loss: 6.890\n",
      "  Batch: 3\n",
      "Batch 3 Loss: 6.814\n",
      "  Batch: 4\n",
      "Batch 4 Loss: 6.856\n",
      "  Batch: 5\n",
      "Batch 5 Loss: 6.783\n",
      "  Batch: 6\n",
      "Batch 6 Loss: 6.790\n",
      "  Batch: 7\n",
      "Batch 7 Loss: 6.761\n",
      "  Batch: 8\n",
      "Batch 8 Loss: 6.826\n",
      "  Batch: 9\n",
      "Batch 9 Loss: 6.823\n",
      "  Batch: 10\n",
      "Batch 10 Loss: 6.794\n",
      "  Batch: 11\n",
      "Batch 11 Loss: 6.758\n",
      "  Batch: 12\n",
      "Batch 12 Loss: 6.795\n",
      "  Batch: 13\n",
      "Batch 13 Loss: 6.700\n",
      "  Batch: 14\n",
      "Batch 14 Loss: 6.696\n",
      "  Batch: 15\n",
      "Batch 15 Loss: 6.719\n",
      "  Batch: 16\n",
      "Batch 16 Loss: 6.639\n",
      "  Batch: 17\n",
      "Batch 17 Loss: 6.726\n",
      "  Batch: 18\n",
      "Batch 18 Loss: 6.667\n",
      "  Batch: 19\n",
      "Batch 19 Loss: 6.653\n",
      "  Batch: 20\n",
      "Batch 20 Loss: 6.588\n",
      "  Batch: 21\n",
      "Batch 21 Loss: 6.611\n",
      "  Batch: 22\n",
      "Batch 22 Loss: 6.710\n",
      "  Batch: 23\n",
      "Batch 23 Loss: 6.631\n",
      "  Batch: 24\n",
      "Batch 24 Loss: 6.655\n",
      "  Batch: 25\n",
      "Batch 25 Loss: 6.609\n",
      "  Batch: 26\n",
      "Batch 26 Loss: 6.703\n",
      "  Batch: 27\n",
      "Batch 27 Loss: 6.613\n",
      "  Batch: 28\n",
      "Batch 28 Loss: 6.586\n",
      "  Batch: 29\n",
      "Batch 29 Loss: 6.578\n",
      "  Batch: 30\n",
      "Batch 30 Loss: 6.544\n",
      "  Batch: 31\n",
      "Batch 31 Loss: 6.587\n",
      "  Batch: 32\n",
      "Batch 32 Loss: 6.589\n",
      "  Batch: 33\n",
      "Batch 33 Loss: 6.556\n",
      "  Batch: 34\n",
      "Batch 34 Loss: 6.563\n",
      "  Batch: 35\n",
      "Batch 35 Loss: 6.478\n",
      "  Batch: 36\n",
      "Batch 36 Loss: 6.547\n",
      "  Batch: 37\n",
      "Batch 37 Loss: 6.567\n",
      "  Batch: 38\n",
      "Batch 38 Loss: 6.533\n",
      "  Batch: 39\n",
      "Batch 39 Loss: 6.470\n",
      "  Batch: 40\n",
      "Batch 40 Loss: 6.555\n",
      "  Batch: 41\n",
      "Batch 41 Loss: 6.547\n",
      "  Batch: 42\n",
      "Batch 42 Loss: 6.427\n",
      "[Epoch 004] Sup Loss: 6.670\n",
      "Epoch: 4\n",
      "  Batch: 0\n",
      "Batch 0 Loss: 6.416\n",
      "  Batch: 1\n",
      "Batch 1 Loss: 6.459\n",
      "  Batch: 2\n",
      "Batch 2 Loss: 6.467\n",
      "  Batch: 3\n",
      "Batch 3 Loss: 6.403\n",
      "  Batch: 4\n",
      "Batch 4 Loss: 6.488\n",
      "  Batch: 5\n",
      "Batch 5 Loss: 6.450\n",
      "  Batch: 6\n",
      "Batch 6 Loss: 6.475\n",
      "  Batch: 7\n",
      "Batch 7 Loss: 6.445\n",
      "  Batch: 8\n",
      "Batch 8 Loss: 6.402\n",
      "  Batch: 9\n",
      "Batch 9 Loss: 6.395\n",
      "  Batch: 10\n",
      "Batch 10 Loss: 6.392\n",
      "  Batch: 11\n",
      "Batch 11 Loss: 6.381\n",
      "  Batch: 12\n",
      "Batch 12 Loss: 6.430\n",
      "  Batch: 13\n",
      "Batch 13 Loss: 6.369\n",
      "  Batch: 14\n",
      "Batch 14 Loss: 6.452\n",
      "  Batch: 15\n",
      "Batch 15 Loss: 6.335\n",
      "  Batch: 16\n",
      "Batch 16 Loss: 6.489\n",
      "  Batch: 17\n",
      "Batch 17 Loss: 6.449\n",
      "  Batch: 18\n",
      "Batch 18 Loss: 6.373\n",
      "  Batch: 19\n",
      "Batch 19 Loss: 6.406\n",
      "  Batch: 20\n",
      "Batch 20 Loss: 6.383\n",
      "  Batch: 21\n",
      "Batch 21 Loss: 6.394\n",
      "  Batch: 22\n",
      "Batch 22 Loss: 6.315\n",
      "  Batch: 23\n",
      "Batch 23 Loss: 6.350\n",
      "  Batch: 24\n",
      "Batch 24 Loss: 6.365\n",
      "  Batch: 25\n",
      "Batch 25 Loss: 6.392\n",
      "  Batch: 26\n",
      "Batch 26 Loss: 6.257\n",
      "  Batch: 27\n",
      "Batch 27 Loss: 6.359\n",
      "  Batch: 28\n",
      "Batch 28 Loss: 6.242\n",
      "  Batch: 29\n",
      "Batch 29 Loss: 6.276\n",
      "  Batch: 30\n",
      "Batch 30 Loss: 6.333\n",
      "  Batch: 31\n",
      "Batch 31 Loss: 6.327\n",
      "  Batch: 32\n",
      "Batch 32 Loss: 6.278\n",
      "  Batch: 33\n",
      "Batch 33 Loss: 6.341\n",
      "  Batch: 34\n",
      "Batch 34 Loss: 6.250\n",
      "  Batch: 35\n",
      "Batch 35 Loss: 6.335\n",
      "  Batch: 36\n",
      "Batch 36 Loss: 6.252\n",
      "  Batch: 37\n",
      "Batch 37 Loss: 6.294\n",
      "  Batch: 38\n",
      "Batch 38 Loss: 6.300\n",
      "  Batch: 39\n",
      "Batch 39 Loss: 6.330\n",
      "  Batch: 40\n",
      "Batch 40 Loss: 6.342\n",
      "  Batch: 41\n",
      "Batch 41 Loss: 6.385\n",
      "  Batch: 42\n",
      "Batch 42 Loss: 6.260\n",
      "[Epoch 005] Sup Loss: 6.368\n",
      "Epoch: 5\n",
      "  Batch: 0\n",
      "Batch 0 Loss: 6.309\n",
      "  Batch: 1\n",
      "Batch 1 Loss: 6.264\n",
      "  Batch: 2\n",
      "Batch 2 Loss: 6.330\n",
      "  Batch: 3\n",
      "Batch 3 Loss: 6.253\n",
      "  Batch: 4\n",
      "Batch 4 Loss: 6.231\n",
      "  Batch: 5\n",
      "Batch 5 Loss: 6.238\n",
      "  Batch: 6\n",
      "Batch 6 Loss: 6.186\n",
      "  Batch: 7\n",
      "Batch 7 Loss: 6.192\n",
      "  Batch: 8\n",
      "Batch 8 Loss: 6.248\n",
      "  Batch: 9\n",
      "Batch 9 Loss: 6.153\n",
      "  Batch: 10\n",
      "Batch 10 Loss: 6.244\n",
      "  Batch: 11\n",
      "Batch 11 Loss: 6.251\n",
      "  Batch: 12\n",
      "Batch 12 Loss: 6.288\n",
      "  Batch: 13\n",
      "Batch 13 Loss: 6.225\n",
      "  Batch: 14\n",
      "Batch 14 Loss: 6.220\n",
      "  Batch: 15\n",
      "Batch 15 Loss: 6.263\n",
      "  Batch: 16\n",
      "Batch 16 Loss: 6.288\n",
      "  Batch: 17\n",
      "Batch 17 Loss: 6.290\n",
      "  Batch: 18\n",
      "Batch 18 Loss: 6.159\n",
      "  Batch: 19\n",
      "Batch 19 Loss: 6.085\n",
      "  Batch: 20\n",
      "Batch 20 Loss: 6.186\n",
      "  Batch: 21\n",
      "Batch 21 Loss: 6.294\n",
      "  Batch: 22\n",
      "Batch 22 Loss: 6.189\n",
      "  Batch: 23\n",
      "Batch 23 Loss: 6.164\n",
      "  Batch: 24\n",
      "Batch 24 Loss: 6.124\n",
      "  Batch: 25\n",
      "Batch 25 Loss: 6.143\n",
      "  Batch: 26\n",
      "Batch 26 Loss: 6.129\n",
      "  Batch: 27\n",
      "Batch 27 Loss: 6.166\n",
      "  Batch: 28\n",
      "Batch 28 Loss: 6.239\n",
      "  Batch: 29\n",
      "Batch 29 Loss: 6.187\n",
      "  Batch: 30\n",
      "Batch 30 Loss: 6.147\n",
      "  Batch: 31\n",
      "Batch 31 Loss: 6.152\n",
      "  Batch: 32\n",
      "Batch 32 Loss: 6.214\n",
      "  Batch: 33\n",
      "Batch 33 Loss: 6.135\n",
      "  Batch: 34\n",
      "Batch 34 Loss: 6.217\n",
      "  Batch: 35\n",
      "Batch 35 Loss: 6.078\n",
      "  Batch: 36\n",
      "Batch 36 Loss: 6.180\n",
      "  Batch: 37\n",
      "Batch 37 Loss: 6.200\n",
      "  Batch: 38\n",
      "Batch 38 Loss: 6.128\n",
      "  Batch: 39\n",
      "Batch 39 Loss: 6.188\n",
      "  Batch: 40\n",
      "Batch 40 Loss: 6.098\n",
      "  Batch: 41\n",
      "Batch 41 Loss: 6.239\n",
      "  Batch: 42\n",
      "Batch 42 Loss: 6.150\n",
      "[Epoch 006] Sup Loss: 6.202\n",
      "Epoch: 6\n",
      "  Batch: 0\n",
      "Batch 0 Loss: 6.211\n",
      "  Batch: 1\n",
      "Batch 1 Loss: 6.055\n",
      "  Batch: 2\n",
      "Batch 2 Loss: 6.140\n",
      "  Batch: 3\n",
      "Batch 3 Loss: 6.163\n",
      "  Batch: 4\n",
      "Batch 4 Loss: 6.128\n",
      "  Batch: 5\n",
      "Batch 5 Loss: 6.095\n",
      "  Batch: 6\n",
      "Batch 6 Loss: 6.147\n",
      "  Batch: 7\n",
      "Batch 7 Loss: 6.117\n",
      "  Batch: 8\n",
      "Batch 8 Loss: 6.020\n",
      "  Batch: 9\n",
      "Batch 9 Loss: 6.123\n",
      "  Batch: 10\n",
      "Batch 10 Loss: 6.096\n",
      "  Batch: 11\n",
      "Batch 11 Loss: 6.128\n",
      "  Batch: 12\n",
      "Batch 12 Loss: 6.116\n",
      "  Batch: 13\n",
      "Batch 13 Loss: 6.101\n",
      "  Batch: 14\n",
      "Batch 14 Loss: 6.059\n",
      "  Batch: 15\n",
      "Batch 15 Loss: 6.114\n",
      "  Batch: 16\n",
      "Batch 16 Loss: 6.073\n",
      "  Batch: 17\n",
      "Batch 17 Loss: 6.057\n",
      "  Batch: 18\n",
      "Batch 18 Loss: 6.127\n",
      "  Batch: 19\n",
      "Batch 19 Loss: 6.023\n",
      "  Batch: 20\n",
      "Batch 20 Loss: 6.070\n",
      "  Batch: 21\n",
      "Batch 21 Loss: 6.113\n",
      "  Batch: 22\n",
      "Batch 22 Loss: 6.095\n",
      "  Batch: 23\n",
      "Batch 23 Loss: 6.049\n",
      "  Batch: 24\n",
      "Batch 24 Loss: 6.060\n",
      "  Batch: 25\n",
      "Batch 25 Loss: 6.094\n",
      "  Batch: 26\n",
      "Batch 26 Loss: 6.033\n",
      "  Batch: 27\n",
      "Batch 27 Loss: 6.072\n",
      "  Batch: 28\n",
      "Batch 28 Loss: 6.079\n",
      "  Batch: 29\n",
      "Batch 29 Loss: 6.061\n",
      "  Batch: 30\n",
      "Batch 30 Loss: 6.025\n",
      "  Batch: 31\n",
      "Batch 31 Loss: 6.130\n",
      "  Batch: 32\n",
      "Batch 32 Loss: 6.076\n",
      "  Batch: 33\n",
      "Batch 33 Loss: 5.965\n",
      "  Batch: 34\n",
      "Batch 34 Loss: 6.112\n",
      "  Batch: 35\n",
      "Batch 35 Loss: 5.999\n",
      "  Batch: 36\n",
      "Batch 36 Loss: 6.019\n",
      "  Batch: 37\n",
      "Batch 37 Loss: 6.117\n",
      "  Batch: 38\n",
      "Batch 38 Loss: 5.906\n",
      "  Batch: 39\n",
      "Batch 39 Loss: 5.981\n",
      "  Batch: 40\n",
      "Batch 40 Loss: 5.999\n",
      "  Batch: 41\n",
      "Batch 41 Loss: 6.050\n",
      "  Batch: 42\n",
      "Batch 42 Loss: 6.158\n",
      "[Epoch 007] Sup Loss: 6.078\n",
      "Epoch: 7\n",
      "  Batch: 0\n",
      "Batch 0 Loss: 6.002\n",
      "  Batch: 1\n",
      "Batch 1 Loss: 6.001\n",
      "  Batch: 2\n",
      "Batch 2 Loss: 6.021\n",
      "  Batch: 3\n",
      "Batch 3 Loss: 6.045\n",
      "  Batch: 4\n",
      "Batch 4 Loss: 6.009\n",
      "  Batch: 5\n",
      "Batch 5 Loss: 6.071\n",
      "  Batch: 6\n",
      "Batch 6 Loss: 6.003\n",
      "  Batch: 7\n",
      "Batch 7 Loss: 6.030\n",
      "  Batch: 8\n",
      "Batch 8 Loss: 6.014\n",
      "  Batch: 9\n",
      "Batch 9 Loss: 5.981\n",
      "  Batch: 10\n",
      "Batch 10 Loss: 5.993\n",
      "  Batch: 11\n",
      "Batch 11 Loss: 6.040\n",
      "  Batch: 12\n",
      "Batch 12 Loss: 6.044\n",
      "  Batch: 13\n",
      "Batch 13 Loss: 5.896\n",
      "  Batch: 14\n",
      "Batch 14 Loss: 5.988\n",
      "  Batch: 15\n",
      "Batch 15 Loss: 5.972\n",
      "  Batch: 16\n",
      "Batch 16 Loss: 6.005\n",
      "  Batch: 17\n",
      "Batch 17 Loss: 6.030\n",
      "  Batch: 18\n",
      "Batch 18 Loss: 6.025\n",
      "  Batch: 19\n",
      "Batch 19 Loss: 5.917\n",
      "  Batch: 20\n",
      "Batch 20 Loss: 5.949\n",
      "  Batch: 21\n",
      "Batch 21 Loss: 6.000\n",
      "  Batch: 22\n",
      "Batch 22 Loss: 5.942\n",
      "  Batch: 23\n",
      "Batch 23 Loss: 5.960\n",
      "  Batch: 24\n",
      "Batch 24 Loss: 6.013\n",
      "  Batch: 25\n",
      "Batch 25 Loss: 5.921\n",
      "  Batch: 26\n",
      "Batch 26 Loss: 5.887\n",
      "  Batch: 27\n",
      "Batch 27 Loss: 6.034\n",
      "  Batch: 28\n",
      "Batch 28 Loss: 5.966\n",
      "  Batch: 29\n",
      "Batch 29 Loss: 5.940\n",
      "  Batch: 30\n",
      "Batch 30 Loss: 5.968\n",
      "  Batch: 31\n",
      "Batch 31 Loss: 5.956\n",
      "  Batch: 32\n",
      "Batch 32 Loss: 5.953\n",
      "  Batch: 33\n",
      "Batch 33 Loss: 5.961\n",
      "  Batch: 34\n",
      "Batch 34 Loss: 5.959\n",
      "  Batch: 35\n",
      "Batch 35 Loss: 5.933\n",
      "  Batch: 36\n",
      "Batch 36 Loss: 5.918\n",
      "  Batch: 37\n",
      "Batch 37 Loss: 6.014\n",
      "  Batch: 38\n",
      "Batch 38 Loss: 5.839\n",
      "  Batch: 39\n",
      "Batch 39 Loss: 5.964\n",
      "  Batch: 40\n",
      "Batch 40 Loss: 5.934\n",
      "  Batch: 41\n",
      "Batch 41 Loss: 5.815\n",
      "  Batch: 42\n",
      "Batch 42 Loss: 5.978\n",
      "[Epoch 008] Sup Loss: 5.974\n",
      "Epoch: 8\n",
      "  Batch: 0\n",
      "Batch 0 Loss: 5.879\n",
      "  Batch: 1\n",
      "Batch 1 Loss: 5.910\n",
      "  Batch: 2\n",
      "Batch 2 Loss: 5.997\n",
      "  Batch: 3\n",
      "Batch 3 Loss: 5.978\n",
      "  Batch: 4\n",
      "Batch 4 Loss: 5.818\n",
      "  Batch: 5\n",
      "Batch 5 Loss: 5.777\n",
      "  Batch: 6\n",
      "Batch 6 Loss: 5.962\n",
      "  Batch: 7\n",
      "Batch 7 Loss: 5.876\n",
      "  Batch: 8\n",
      "Batch 8 Loss: 5.954\n",
      "  Batch: 9\n",
      "Batch 9 Loss: 5.914\n",
      "  Batch: 10\n",
      "Batch 10 Loss: 5.873\n",
      "  Batch: 11\n",
      "Batch 11 Loss: 5.854\n",
      "  Batch: 12\n",
      "Batch 12 Loss: 5.917\n",
      "  Batch: 13\n",
      "Batch 13 Loss: 5.902\n",
      "  Batch: 14\n",
      "Batch 14 Loss: 5.901\n",
      "  Batch: 15\n",
      "Batch 15 Loss: 5.949\n",
      "  Batch: 16\n",
      "Batch 16 Loss: 5.898\n",
      "  Batch: 17\n",
      "Batch 17 Loss: 5.832\n",
      "  Batch: 18\n",
      "Batch 18 Loss: 5.913\n",
      "  Batch: 19\n",
      "Batch 19 Loss: 5.927\n",
      "  Batch: 20\n",
      "Batch 20 Loss: 5.895\n",
      "  Batch: 21\n",
      "Batch 21 Loss: 5.851\n",
      "  Batch: 22\n",
      "Batch 22 Loss: 5.899\n",
      "  Batch: 23\n",
      "Batch 23 Loss: 5.928\n",
      "  Batch: 24\n",
      "Batch 24 Loss: 5.867\n",
      "  Batch: 25\n",
      "Batch 25 Loss: 5.891\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "# Training\n",
    "m2 = M2(input_size, z_dim, num_classes, hidden_dim, seq_len, vocab_size, batch_size).to(device)\n",
    "\n",
    "m2.apply(init_weights)\n",
    "\n",
    "optim = torch.optim.Adam(params=m2.parameters(), lr=0.0001)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    m2.train()\n",
    "    \n",
    "    epoch_losses_sup = 0.0\n",
    "    #for i in tqdm(range(num_batch)):\n",
    "    for batch_idx, (_, xs, ys) in enumerate(train_loader):\n",
    "        #xs, ys = next(train_loader)\n",
    "        xs = xs.view(xs.shape[0], seq_len, input_size).to(device)\n",
    "        ys = ys.to(device)\n",
    "        loss, llk, cat, kld_norm = m2.sup(xs, ys)\n",
    "        optim.zero_grad() \n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        epoch_losses_sup += loss.detach().item()\n",
    "        #print(f\"Batch {batch_idx} Loss: {loss.detach().item():.3f}\")\n",
    "\n",
    "        gc.collect()\n",
    "    \n",
    "    avg_loss = epoch_losses_sup / len(train_loader)\n",
    "    print(f\"[Epoch {epoch+1:03d}] Sup Loss: {avg_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4939)\n"
     ]
    }
   ],
   "source": [
    "#Accuracy\n",
    "m2.eval() \n",
    "\n",
    "test_acc = m2.accuracy(test_loader)\n",
    "\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mva_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
